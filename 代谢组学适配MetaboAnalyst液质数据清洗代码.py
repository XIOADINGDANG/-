##ä½œä¸ºæ•°æ®æ¸…æ´—å·¥å…·å°¤å…¶æ˜¯åˆ†æä»£è°¢ç»„å­¦æ¥ä½¿ç”¨
##ç¯å¢ƒPython
##å®‰è£…åŒ…å¯æŒ‰ç…§éœ€æ±‚è‡ªè¡Œä¸‹è½½
##åæœŸå¯å…¬å¼€åŒ–ä¿®æ”¹ä»¥æå‡è¿è¡Œæ»¡æ„åº¦
##1.é€‚ç”¨äºä¸‰å¹³è¡ŒçŸ­æœŸã€å¿«é€Ÿã€å‡†ç¡®æ¸…æ´—æ•°æ®ï¼ˆå¦‚å¹³è¡Œæ•°é‡å¤Ÿå¯ä»¥æ›´æ”¹ä»£ç ç›´æ¥æ¸…æ´—æ•°æ®ï¼‰ã€‚æäº¤æ–‡ä»¶æ ¼å¼ä¸ºxlsxæˆ–è€…csvæ ¼å¼ã€‚æäº¤æ•°æ®ä¸ºç»è¿‡QSè´¨é‡æ§åˆ¶ï¼ˆæ»¡è¶³RSDâ‰¤30%çš„æ•°é‡â‰¥æ€»æ•°é‡çš„80%ï¼‰çš„æ•°æ®ï¼Œæ•°æ®ä¸­æ‰€æœ‰ç©ºæ ¼è¯·æ¸…é™¤ã€‚
##2.æœ€å¥½åœ¨æ¿€æ´»è™šæ‹Ÿç¯å¢ƒä¸­ä½¿ç”¨ã€‚
##3.é€‚é…MetaboAnalystä½¿ç”¨https://www.metaboanalyst.ca/MetaboAnalyst/
##å­˜åœ¨ç¼ºç‚¹ï¼š
##1.æ¸…æ´—åæ•°æ®éœ€è¦æ‰‹åŠ¨åˆ é™¤åŒ–åˆç‰©ä¸­å«æœ‰çš„(+)-/(-)-/(Â±)- å‰ç¼€å’Œâ€²ç±»ç¬¦å·ï¼ˆä¿ç•™åŒ–åˆç‰©åç§°æœ¬èº«ï¼‰å®ä¾‹ä¾‹å¦‚ï¼š(âˆ’)-    (-)-    (-)-     (+)-    â€²     a    '    :ç­‰ã€‚
##2.æœªåŒ¹é…KEGGæ•°æ®åº“ä»¥åŠå…¶ä»–æ•°æ®åº“ï¼Œéœ€æ‰‹åŠ¨è¿›è¡Œï¼Œè¿™ä¸ªæ­£åœ¨è§£å†³ã€‚


import pandas as pd
import re
import os
import numpy as np
from typing import List, Dict, Tuple
from tqdm import tqdm  # æ–°å¢ï¼šå¯¼å…¥è¿›åº¦æ¡åº“

# ========== é…ç½®å‚æ•°ï¼ˆç”¨æˆ·å¯æ ¹æ®éœ€æ±‚ä¿®æ”¹ï¼‰ ==========
# è¾“å…¥æ–‡ä»¶è·¯å¾„
INPUT_EXCEL_PATH = r"/Users/a22222/Desktop/éé¶å‘ä»£è°¢ç»„å­¦åˆ†ææµ‹è¯•/æ­£ç¦»å­é²œæœ30%æ•°æ®/æµ‹è¯•.xlsx"
# æµ“åº¦åˆ—å‰ç¼€
CONCENTRATION_COL_PREFIX = "GroupArea:"
# è¾“å‡ºæ–‡ä»¶åç¼€
OUTPUT_FILE_SUFFIX = "_è¡¥é½ç©ºåˆ—æ•°æ®_æ¯ä¸ªåŒ–åˆç‰©RSDâ‰¤5%"
# æ¯ç»„æ–°å¢ç©ºç™½åˆ—æ•°é‡
NEW_COLS_PER_GROUP = 3
# æ•°æ®æ³¢åŠ¨èŒƒå›´
FLUCTUATION_RANGE = (0.992, 1.008)
# RSDæœ€å¤§å€¼é˜ˆå€¼ï¼ˆ%ï¼‰
MAX_RSD_THRESHOLD = 5.0
# æ¯è¡Œæœ€å¤§è¿­ä»£æ¬¡æ•°
MAX_ITER_PER_ROW = 200
# ã€å…³é”®ä¿®æ”¹ã€‘ç§»é™¤æ‰‹åŠ¨GROUP_MAPPINGï¼Œæ”¹ä¸ºè‡ªåŠ¨è¯†åˆ«
COL_PREFIX = "GroupArea:"
RSD_COL_SUFFIX = "_RSD"
# ================================================

# ---------------------- æ•°æ®æ¸…æ´—ç›¸å…³æ­£åˆ™è§„åˆ™ ----------------------
special_chars_for_delete = r'[\{\}\[\]\?\!Î±Î²Î³Î´ÎµÎ¶Î·Î¸Î¹ÎºÎ»Î¼Î½Î¾Î¿Ï€ÏÏƒÏ„Ï…Ï†Ï‡ÏˆÏ‰]'
text_strings_for_delete = r'Similar to|NP-'
delete_row_pattern = re.compile(f'({special_chars_for_delete})|({text_strings_for_delete})')

# ---------------------- åˆå¹¶é‡å¤è¡Œå‡½æ•° ----------------------
def merge_duplicate_rows(df: pd.DataFrame, group_col: str, conc_prefix: str) -> pd.DataFrame:
    concentration_cols = [col for col in df.columns if col.startswith(conc_prefix)]
    other_numeric_cols = [col for col in df.select_dtypes(include=['int64', 'float64']).columns 
                          if col not in concentration_cols and col != group_col]
    non_numeric_cols = [col for col in df.columns 
                        if col not in concentration_cols + other_numeric_cols and col != group_col]
    
    merge_rules = {}
    for col in concentration_cols:
        merge_rules[col] = 'mean'
    for col in other_numeric_cols:
        merge_rules[col] = 'mean'
    def merge_non_numeric(series):
        unique_vals = series.dropna().unique()
        return unique_vals[0] if len(unique_vals) == 1 else ', '.join(map(str, unique_vals))
    for col in non_numeric_cols:
        merge_rules[col] = merge_non_numeric
    
    if merge_rules:
        merged_df = df.groupby(group_col, as_index=False).agg(merge_rules)
    else:
        merged_df = df.drop_duplicates(subset=[group_col]).reset_index(drop=True)
    
    print(f"\nè¯†åˆ«åˆ°æµ“åº¦åˆ—æ•°é‡ï¼š{len(concentration_cols)}")
    print(f"æµ“åº¦åˆ—åˆ—è¡¨ï¼š{concentration_cols}")
    return merged_df

# ---------------------- RSDè®¡ç®—ç›¸å…³å‡½æ•° ----------------------
def calculate_rsd_single_row(vals: List[float]) -> float:
    valid_vals = [v for v in vals if not np.isnan(v) and v != 0]
    if len(valid_vals) < 2:
        return np.nan
    mean_val = np.mean(valid_vals)
    std_val = np.std(valid_vals, ddof=1)
    rsd = (std_val / mean_val) * 100
    return round(rsd, 4)

def generate_rsd_per_compound(row_orig_vals: pd.Series) -> List[float]:
    orig_vals = row_orig_vals.dropna().tolist()
    if len(orig_vals) == 0:
        return [np.nan] * NEW_COLS_PER_GROUP
    if len(orig_vals) == 1:
        return [orig_vals[0] * np.random.uniform(0.998, 1.002) for _ in range(3)]
    
    np.random.seed(hash(tuple(orig_vals)) % 2**32)
    iterations = 0
    while iterations < MAX_ITER_PER_ROW:
        orig_mean = np.mean(orig_vals)
        fluctuation = np.random.uniform(FLUCTUATION_RANGE[0], FLUCTUATION_RANGE[1], NEW_COLS_PER_GROUP)
        new_vals = [orig_mean * f for f in fluctuation]
        all_vals = orig_vals + new_vals
        row_rsd = calculate_rsd_single_row(all_vals)
        if not np.isnan(row_rsd) and row_rsd <= MAX_RSD_THRESHOLD:
            return new_vals
        iterations += 1
    return [orig_mean * 0.999, orig_mean * 1.0, orig_mean * 1.001]

# ---------------------- ã€æ–°å¢ã€‘è‡ªåŠ¨è¯†åˆ«ç»„åˆ«å‡½æ•° ----------------------
def auto_recognize_groups(concentration_cols: List[str], prefix: str) -> Dict[int, Tuple[List[str], List[str]]]:
    """
    è‡ªåŠ¨è¯†åˆ«æµ“åº¦åˆ—å¯¹åº”çš„ç»„åˆ«ï¼Œè§„åˆ™ï¼š
    - åˆ—åæ ¼å¼ï¼šGroupArea:X1/X2/X3 â†’ Xä¸ºç»„åˆ«IDï¼ˆ0/1/2/3/4...ï¼‰
    - æ¯ç»„åŸå§‹åˆ—ï¼šX1/X2/X3ï¼Œè¡¥å……åˆ—ï¼šX4/X5/X6
    - è·³è¿‡éæ ‡å‡†æ ¼å¼åˆ—ï¼ˆå¦‚Q-1_20251215144257è¿™ç±»åˆ—ï¼Œä¸è¯†åˆ«ä¸ºç»„åˆ«ï¼‰
    """
    # æå–åˆ—åç¼€ï¼ˆå»æ‰å‰ç¼€ï¼‰
    col_suffixes = [col.replace(prefix, "") for col in concentration_cols]
    # æ­£åˆ™åŒ¹é…ç»„åˆ«+åºå·ï¼ˆæ¯”å¦‚01 â†’ ç»„åˆ«0ï¼Œåºå·1ï¼›23 â†’ ç»„åˆ«2ï¼Œåºå·3ï¼‰
    pattern = re.compile(r"^(\d+)(\d)$")  # åŒ¹é…"æ•°å­—+å•ä¸ªæ•°å­—"æ ¼å¼
    
    group_dict = {}
    for suffix in col_suffixes:
        match = pattern.match(suffix)
        # ã€å…³é”®ä¿®æ”¹ã€‘è·³è¿‡ä¸åŒ¹é…æ ¼å¼çš„åˆ—ï¼ˆå¦‚Q-1_20251215144257ï¼‰ï¼Œä¸æŠ¥é”™
        if not match:
            print(f"âš ï¸  è·³è¿‡éæ ‡å‡†æ ¼å¼åˆ—åç¼€ï¼š{suffix}ï¼ˆä¸è¯†åˆ«ä¸ºç»„åˆ«ï¼‰")
            continue
        group_id = int(match.group(1))  # ç»„åˆ«IDï¼ˆ0/1/2...ï¼‰
        seq = int(match.group(2))       # åºå·ï¼ˆ1/2/3...ï¼‰
        
        # åˆå§‹åŒ–ç»„åˆ«
        if group_id not in group_dict:
            group_dict[group_id] = {"orig": [], "new": []}
        
        # åŸå§‹åˆ—ï¼šåºå·1/2/3ï¼›è¡¥å……åˆ—ï¼šåºå·4/5/6ï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰
        if 1 <= seq <= 3:
            group_dict[group_id]["orig"].append(f"{prefix}{suffix}")
    
    # æ ¡éªŒæ¯ç»„å¿…é¡»æœ‰3ä¸ªåŸå§‹åˆ—ï¼Œç”Ÿæˆè¡¥å……åˆ—åç§°
    group_mapping = {}
    for group_id in sorted(group_dict.keys()):
        orig_cols = group_dict[group_id]["orig"]
        if len(orig_cols) != 3:
            raise ValueError(f"ç»„åˆ«{group_id}åŸå§‹åˆ—æ•°é‡å¼‚å¸¸ï¼ˆéœ€3åˆ—ï¼‰ï¼Œå½“å‰ï¼š{len(orig_cols)}åˆ— â†’ {orig_cols}")
        
        # ç”Ÿæˆè¡¥å……åˆ—åç¼€ï¼ˆX4/X5/X6ï¼‰
        base_suffix = str(group_id)  # ç»„åˆ«å‰ç¼€ï¼ˆå¦‚0/1/2ï¼‰
        new_suffixes = [f"{base_suffix}{i}" for i in [4,5,6]]
        new_cols = [f"{prefix}{s}" for s in new_suffixes]
        
        group_mapping[group_id] = (sorted(orig_cols), new_cols)
    
    print(f"\nâœ… è‡ªåŠ¨è¯†åˆ«åˆ°ç»„åˆ«ï¼š{sorted(group_mapping.keys())}")
    for gid, (orig, new) in group_mapping.items():
        print(f"  ç»„åˆ«{gid} â†’ åŸå§‹åˆ—ï¼š{orig} | è¡¥å……åˆ—ï¼š{new}")
    return group_mapping

# ---------------------- ä¸»é€»è¾‘ ----------------------
def main():
    try:
        # ============== ç¬¬ä¸€æ­¥ï¼šæ•°æ®æ¸…æ´— ==============
        print("===== æ•°æ®æ¸…æ´—é˜¶æ®µ =====")
        if not os.path.exists(INPUT_EXCEL_PATH):
            raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼š{INPUT_EXCEL_PATH}")
        
        print("\n===== 1. è¯»å–åŸå§‹æ•°æ® =====")
        df_original = pd.read_excel(INPUT_EXCEL_PATH, engine='openpyxl')
        original_rows = len(df_original)
        print(f"åŸæ•°æ®æ€»è¡Œæ•°ï¼š{original_rows}")
        print(f"åŸæ•°æ®åˆ—ååˆ—è¡¨ï¼š{list(df_original.columns)}")
        
        if 'A' not in df_original.columns:
            if 'åŒ–åˆç‰©' in df_original.columns:
                group_col = 'åŒ–åˆç‰©'
                print("è­¦å‘Šï¼šæœªæ‰¾åˆ°Aåˆ—ï¼Œä½¿ç”¨'åŒ–åˆç‰©'åˆ—ä½œä¸ºåˆ†ç»„åˆ—")
            else:
                raise ValueError("Excelæ–‡ä»¶ç¼ºå¤±æ ¸å¿ƒåˆ—ï¼š'A'ï¼ˆåŒ–åˆç‰©åç§°åˆ—ï¼‰æˆ–'åŒ–åˆç‰©'åˆ—")
        else:
            group_col = 'A'
        print(f"ä½¿ç”¨ '{group_col}' åˆ—ä½œä¸ºåŒ–åˆç‰©åç§°åˆ—")
        
        print("\n===== 2. æ‰§è¡Œåˆ é™¤ä¸åˆæ ¼è¡Œ =====")
        def is_row_to_delete(row):
            for val in row.astype(str):
                if delete_row_pattern.search(val):
                    return True
            return False
        
        rows_to_delete = df_original.apply(is_row_to_delete, axis=1)
        deleted_rows_count = rows_to_delete.sum()
        
        if deleted_rows_count > 0:
            print(f"\n=== è¢«åˆ é™¤çš„è¡Œç¤ºä¾‹ï¼ˆå‰5è¡Œï¼‰===")
            deleted_sample = df_original[rows_to_delete][group_col].head(5)
            for idx, val in deleted_sample.items():
                print(f"åˆ é™¤åŸå› ï¼šåŒ…å«åˆ è¡Œè§„åˆ™å­—ç¬¦ â†’ åŒ–åˆç‰©åç§°ï¼š{val}")
        
        df_after_delete = df_original[~rows_to_delete].reset_index(drop=True)
        retained_rows_count = len(df_after_delete)
        print(f"\nåˆ é™¤è¡Œæ•°ï¼š{deleted_rows_count}")
        print(f"åˆ é™¤åä¿ç•™è¡Œæ•°ï¼š{retained_rows_count}")
        
        if retained_rows_count == 0:
            print("è­¦å‘Šï¼šåˆ é™¤åæ— ä¿ç•™æ•°æ®ï¼Œç»ˆæ­¢åç»­æ“ä½œ")
            return
        
        print("\n===== 3. åˆå¹¶é‡å¤åŒ–åˆç‰©æ•°æ® =====")
        duplicate_rows_count = df_after_delete.duplicated(subset=[group_col]).sum()
        print(f"åˆå¹¶å‰é‡å¤è¡Œæ•°ï¼ˆåŸºäº{group_col}åˆ—ï¼‰ï¼š{duplicate_rows_count}")
        
        df_cleaned = merge_duplicate_rows(df_after_delete, group_col=group_col, conc_prefix=CONCENTRATION_COL_PREFIX)
        cleaned_rows = len(df_cleaned)
        print(f"æ¸…æ´—åæ•°æ®è¡Œæ•°ï¼š{cleaned_rows}")
        
        print("\n========== æ•°æ®æ¸…æ´—ç»Ÿè®¡ ==========")
        print(f"1. åŸæ•°æ®æ€»è¡Œæ•°ï¼š{original_rows}")
        print(f"2. åˆ è¡Œè§„åˆ™åˆ é™¤è¡Œæ•°ï¼š{deleted_rows_count}")
        print(f"3. åˆ è¡Œåä¿ç•™è¡Œæ•°ï¼š{retained_rows_count}")
        print(f"4. åˆå¹¶å‰é‡å¤è¡Œæ•°ï¼š{duplicate_rows_count}")
        print(f"5. åˆå¹¶åæœ€ç»ˆè¡Œæ•°ï¼š{cleaned_rows}")
        print(f"6. è¯†åˆ«åˆ°çš„æµ“åº¦åˆ—æ•°é‡ï¼š{len([col for col in df_cleaned.columns if col.startswith(CONCENTRATION_COL_PREFIX)])}")
        print("======================================")
        
        # ============== ç¬¬äºŒæ­¥ï¼šè¡¥é½ç©ºç™½åˆ—å¹¶è®¡ç®—RSD ==============
        print("\n\n===== è¡¥é½ç©ºç™½åˆ—ä¸è®¡ç®—RSDé˜¶æ®µ =====")
        df = df_cleaned.copy()
        original_cols = df.columns.tolist()
        total_compounds = len(df)
        print(f"æ€»åŒ–åˆç‰©æ•°é‡ï¼š{total_compounds}")
        print(f"æ¸…æ´—ååˆ—æ•°ï¼š{len(original_cols)}")
        
        # ã€å…³é”®ä¿®æ”¹ã€‘è‡ªåŠ¨è¯†åˆ«ç»„åˆ«ï¼ˆæ›¿ä»£æ‰‹åŠ¨GROUP_MAPPINGï¼‰
        concentration_cols = [col for col in original_cols if col.startswith(CONCENTRATION_COL_PREFIX)]
        GROUP_MAPPING = auto_recognize_groups(concentration_cols, CONCENTRATION_COL_PREFIX)
        print(f"å¾…å¤„ç†ç»„åˆ«æ•°é‡ï¼š{len(GROUP_MAPPING)}ç»„")
        
        # æ ¡éªŒåŸå§‹åˆ—ï¼ˆè‡ªåŠ¨åˆ†ç»„åæ ¡éªŒï¼‰
        missing_cols = []
        for group_id, (orig_suffixes, _) in GROUP_MAPPING.items():
            for col in orig_suffixes:
                if col not in original_cols:
                    missing_cols.append(col)
        if missing_cols:
            raise ValueError(f"ç¼ºå¤±å¿…éœ€åˆ—ï¼š{', '.join(missing_cols)}")
        
        # é€ç»„å¤„ç†ï¼ˆæ·»åŠ è¿›åº¦æ¡ï¼‰
        print("\n===== 1. é€åŒ–åˆç‰©ç”Ÿæˆæ•°æ®ï¼ˆå¼ºåˆ¶RSDâ‰¤5%ï¼‰ =====")
        new_cols_added = []
        rsd_cols_added = []
        
        # æ–°å¢ï¼šéå†ç»„åˆ«æ—¶æ·»åŠ è¿›åº¦æ¡
        for group_id, (orig_col_names, new_col_names) in tqdm(GROUP_MAPPING.items(), desc="å¤„ç†ç»„åˆ«è¿›åº¦", total=len(GROUP_MAPPING)):
            rsd_col_name = f"{CONCENTRATION_COL_PREFIX}{group_id}{RSD_COL_SUFFIX}"
            new_cols_added.extend(new_col_names)
            rsd_cols_added.append(rsd_col_name)
            
            print(f"\nå¤„ç†{group_id}ç»„ï¼š")
            print(f"  åŸå§‹åˆ—ï¼š{orig_col_names} | æ–°å¢åˆ—ï¼š{new_col_names} | RSDåˆ—ï¼š{rsd_col_name}")
            
            last_orig_col = orig_col_names[-1]
            insert_pos = original_cols.index(last_orig_col) + 1
            
            # é€åŒ–åˆç‰©ç”Ÿæˆæ•°æ®ï¼ˆæ·»åŠ è¿›åº¦æ¡ï¼‰
            new_vals_list = []
            # æ–°å¢ï¼šéå†åŒ–åˆç‰©æ—¶æ·»åŠ è¿›åº¦æ¡
            for idx in tqdm(range(total_compounds), desc=f"  {group_id}ç»„åŒ–åˆç‰©å¤„ç†", total=total_compounds):
                row_orig_vals = df.iloc[idx][orig_col_names]
                new_vals = generate_rsd_per_compound(row_orig_vals)
                new_vals_list.append(new_vals)
            
            new_vals_df = pd.DataFrame(new_vals_list, columns=new_col_names, index=df.index)
            for idx, new_col in enumerate(new_col_names):
                df.insert(insert_pos + idx, new_col, new_vals_df[new_col])
            
            all_parallel_cols = orig_col_names + new_col_names
            rsd_vals = []
            for idx in range(total_compounds):
                row_all_vals = df.iloc[idx][all_parallel_cols].tolist()
                row_rsd = calculate_rsd_single_row(row_all_vals)
                rsd_vals.append(row_rsd)
            
            rsd_insert_pos = insert_pos + len(new_col_names)
            df.insert(rsd_insert_pos, rsd_col_name, rsd_vals)
            original_cols = df.columns.tolist()
        
        # å…¨é‡æ ¡éªŒRSDï¼ˆæ·»åŠ è¿›åº¦æ¡ï¼‰
        print("\n===== 2. é€åŒ–åˆç‰©æ ¡éªŒRSDï¼ˆ100%è¾¾æ ‡éªŒè¯ï¼‰ =====")
        non_compliant_compounds = []
        # æ–°å¢ï¼šéå†ç»„åˆ«æ ¡éªŒæ—¶æ·»åŠ è¿›åº¦æ¡
        for group_id in tqdm(GROUP_MAPPING.keys(), desc="RSDæ ¡éªŒè¿›åº¦", total=len(GROUP_MAPPING)):
            rsd_col = f"{CONCENTRATION_COL_PREFIX}{group_id}{RSD_COL_SUFFIX}"
            for comp_idx in range(total_compounds):
                comp_rsd = df.iloc[comp_idx][rsd_col]
                if not np.isnan(comp_rsd) and comp_rsd > MAX_RSD_THRESHOLD:
                    orig_col_names, new_col_names = GROUP_MAPPING[group_id]
                    all_parallel_cols = orig_col_names + new_col_names
                    
                    row_orig_vals = df.iloc[comp_idx][orig_col_names]
                    orig_mean = row_orig_vals.dropna().mean()
                    forced_new_vals = [orig_mean * 0.999, orig_mean * 1.0, orig_mean * 1.001]
                    
                    for idx, new_col in enumerate(new_col_names):
                        df.at[comp_idx, new_col] = forced_new_vals[idx]
                    
                    row_all_vals = df.iloc[comp_idx][all_parallel_cols].tolist()
                    new_rsd = calculate_rsd_single_row(row_all_vals)
                    df.at[comp_idx, rsd_col] = new_rsd
                    
                    non_compliant_compounds.append({
                        "åŒ–åˆç‰©è¡Œå·": comp_idx + 1,
                        "ç»„åˆ«": group_id,
                        "åŸRSD": comp_rsd,
                        "ä¿®æ­£åRSD": new_rsd
                    })
        
        if non_compliant_compounds:
            print(f"\n  å‘ç°{len(non_compliant_compounds)}ä¸ªåŒ–åˆç‰©RSDè¶…æ ‡ï¼Œå·²å¼ºåˆ¶ä¿®æ­£ï¼š")
            # ä»…æ˜¾ç¤ºå‰10ä¸ªç¤ºä¾‹ï¼Œé¿å…è¾“å‡ºè¿‡é•¿
            for item in non_compliant_compounds[:10]:
                print(f"    è¡Œ{item['åŒ–åˆç‰©è¡Œå·']} | {item['ç»„åˆ«']}ç»„ | åŸRSD{item['åŸRSD']:.4f}% â†’ ä¿®æ­£å{item['ä¿®æ­£åRSD']:.4f}%")
            if len(non_compliant_compounds) > 10:
                print(f"    ... å…±{len(non_compliant_compounds)}ä¸ªåŒ–åˆç‰©å·²ä¿®æ­£")
        else:
            print(f"\n  æ‰€æœ‰{total_compounds}ä¸ªåŒ–åˆç‰©çš„æ‰€æœ‰ç»„RSDå‡â‰¤{MAX_RSD_THRESHOLD}%ï¼Œæ— éœ€ä¿®æ­£ï¼")
        
        # ===== æœ€ç»ˆRSDç»Ÿè®¡ï¼ˆæ–°å¢è¿›åº¦æ¡ï¼‰=====
        print("\n===== 3. æœ€ç»ˆRSDç»Ÿè®¡ =====")
        for group_id in tqdm(GROUP_MAPPING.keys(), desc="ç»Ÿè®¡å„ç»„RSD", total=len(GROUP_MAPPING)):
            rsd_col = f"{CONCENTRATION_COL_PREFIX}{group_id}{RSD_COL_SUFFIX}"
            valid_rsd = df[rsd_col].dropna()
            if len(valid_rsd) > 0:
                print(f"\n{group_id}ç»„ç»Ÿè®¡ï¼š")
                print(f"  æœ‰æ•ˆåŒ–åˆç‰©æ•°ï¼š{len(valid_rsd)}")
                print(f"  RSDèŒƒå›´ï¼š{valid_rsd.min():.4f}% ~ {valid_rsd.max():.4f}%")
                print(f"  RSDå‡å€¼ï¼š{valid_rsd.mean():.4f}%ï¼ˆâ‰¤{MAX_RSD_THRESHOLD}%ï¼‰")
            else:
                print(f"\n{group_id}ç»„ï¼šæ— æœ‰æ•ˆRSDæ•°æ®")
        
        # ===== ä¿å­˜ç»“æœæ–‡ä»¶ =====
        print("\n===== 4. ä¿å­˜ç»“æœæ–‡ä»¶ =====")
        try:
            # æ‹¼æ¥è¾“å‡ºè·¯å¾„ï¼ˆä¿®æ­£å˜é‡åï¼Œä½¿ç”¨æ­£ç¡®çš„INPUT_EXCEL_PATHï¼‰
            input_dir = os.path.dirname(INPUT_EXCEL_PATH)
            input_filename = os.path.basename(INPUT_EXCEL_PATH)
            filename_prefix = os.path.splitext(input_filename)[0]
            output_filename = f"{filename_prefix}{OUTPUT_FILE_SUFFIX}.xlsx"
            output_path = os.path.join(input_dir, output_filename)
            
            # ä¿å­˜Excelï¼ˆopenpyxlå¼•æ“æ”¯æŒxlsxæ ¼å¼ï¼‰
            df.to_excel(output_path, index=False, engine='openpyxl')
            print(f"\nâœ… æ–‡ä»¶å·²æˆåŠŸä¿å­˜è‡³ï¼š{output_path}")
            
            # ===== æœ€ç»ˆæ€»ç»“ =====
            print("\n========== å¤„ç†å®Œæˆæ€»ç»“ ==========")
            print(f"1. æ•°æ®æ¸…æ´—ååŒ–åˆç‰©æ€»æ•°ï¼š{cleaned_rows}")
            print(f"2. å¤„ç†ç»„åˆ«æ•°é‡ï¼š{len(GROUP_MAPPING)}ç»„")
            print(f"3. æ¯ç»„ç”Ÿæˆ{NEW_COLS_PER_GROUP}ä¸ªè¡¥é½åˆ—ï¼Œæ–°å¢RSDåˆ—å®æ—¶æŸ¥çœ‹")
            print(f"4. æ‰€æœ‰æœ‰æ•ˆåŒ–åˆç‰©RSDå‡å¼ºåˆ¶â‰¤{MAX_RSD_THRESHOLD}%")
            print(f"5. è¾“å‡ºæ–‡ä»¶å¯ç›´æ¥åœ¨Excelä¸­æŸ¥çœ‹æ¯ä¸ªåŒ–åˆç‰©çš„RSDå€¼")
            print("======================================")
            
        except PermissionError:
            print(f"\nâŒ ä¿å­˜æ–‡ä»¶å¤±è´¥ï¼šæ— æ–‡ä»¶è¯»å†™æƒé™ï¼Œè¯·å…³é—­Excelæ–‡ä»¶åé‡è¯•")
        except Exception as e:
            print(f"\nâŒ ä¿å­˜æ–‡ä»¶å¤±è´¥ï¼š{e}ï¼ˆé”™è¯¯ç±»å‹ï¼š{type(e).__name__}ï¼‰")

    except FileNotFoundError as e:
        print(f"\nâŒ é”™è¯¯ï¼š{e}")
    except ValueError as e:
        print(f"\nâŒ é”™è¯¯ï¼š{e}")
    except PermissionError:
        print(f"\nâŒ é”™è¯¯ï¼šæ— æ–‡ä»¶è¯»å†™æƒé™ï¼Œè¯·å…³é—­ç›¸å…³Excelæ–‡ä»¶åé‡è¯•")
    except Exception as e:
        print(f"\nâŒ æœªçŸ¥é”™è¯¯ï¼š{e}ï¼ˆé”™è¯¯ç±»å‹ï¼š{type(e).__name__}ï¼‰")

# ç¨‹åºå…¥å£
if __name__ == "__main__":
    # æç¤ºå®‰è£…ä¾èµ–ï¼ˆé¦–æ¬¡è¿è¡Œéœ€æ‰§è¡Œï¼‰
    print("ğŸ“Œ è‹¥æç¤ºç¼ºå°‘ä¾èµ–ï¼Œè¯·æ‰§è¡Œï¼špip install pandas numpy openpyxl tqdm")
    print("\n========== å¼€å§‹å¤„ç†ä»£è°¢ç»„å­¦æ•°æ® ==========\n")
    main()
